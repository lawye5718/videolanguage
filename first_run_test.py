#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
VideoLanguage é¦–ç§€æµ‹è¯•è„šæœ¬ (First Run Test)
ä¸€ç«™å¼å…¨é“¾è·¯è§†é¢‘å¤„ç†æµ‹è¯•
"""

import os
import sys
import time
import json
import shutil
from pathlib import Path
from loguru import logger

# æ·»åŠ é¡¹ç›®è·¯å¾„
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

def setup_logging():
    """è®¾ç½®æ—¥å¿—"""
    logger.remove()
    logger.add(sys.stderr, format="{time:YYYY-MM-DD HH:mm:ss} | {level} | {message}", level="INFO")
    logger.add("first_run_test.log", rotation="100 MB", level="DEBUG")

def prepare_test_video():
    """å‡†å¤‡æµ‹è¯•è§†é¢‘"""
    logger.info("ğŸ¬ å‡†å¤‡æµ‹è¯•è§†é¢‘...")
    
    source_video = "/Users/yuanliang/Downloads/p.mp4"
    if not os.path.exists(source_video):
        logger.error(f"âŒ æµ‹è¯•è§†é¢‘ä¸å­˜åœ¨: {source_video}")
        return None
    
    # åˆ›å»ºæµ‹è¯•å·¥ä½œç›®å½•
    test_dir = project_root / "first_run_test"
    test_dir.mkdir(exist_ok=True)
    
    # å¤åˆ¶è§†é¢‘
    test_video = test_dir / "input_video.mp4"
    if not test_video.exists():
        logger.info("ğŸ”„ å¤åˆ¶æµ‹è¯•è§†é¢‘...")
        shutil.copy2(source_video, test_video)
        logger.info(f"âœ… è§†é¢‘å·²å¤åˆ¶åˆ°: {test_video}")
    
    return str(test_video)

def run_demucs_separation(video_path):
    """è¿è¡Œäººå£°åˆ†ç¦»"""
    logger.info("ğŸµ [æ­¥éª¤1] è¿è¡Œäººå£°åˆ†ç¦» (Demucs)...")
    
    try:
        from tools.step010_demucs_simple import separate_all_audio_under_folder
        
        video_dir = Path(video_path).parent
        audio_dir = video_dir / "audio"
        audio_dir.mkdir(exist_ok=True)
        
        # å¤åˆ¶è§†é¢‘æ–‡ä»¶åˆ°audioç›®å½•
        import shutil
        video_copy = audio_dir / "video.mp4"
        if not video_copy.exists():
            shutil.copy2(video_path, video_copy)
        
        logger.info("ğŸ”„ æ‰§è¡Œäººå£°åˆ†ç¦»...")
        start_time = time.time()
        result, vocal_path, instrumental_path = separate_all_audio_under_folder(str(video_dir))
        end_time = time.time()
        
        if result and vocal_path:
            logger.success(f"âœ… äººå£°åˆ†ç¦»å®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’")
            logger.info(f"   äººå£°è½¨é“: {vocal_path}")
            vocal_size = os.path.getsize(vocal_path)
            logger.info(f"   æ–‡ä»¶å¤§å°: {vocal_size / (1024*1024):.2f} MB")
            return vocal_path
        else:
            logger.error("âŒ äººå£°åˆ†ç¦»å¤±è´¥")
            return None
            
    except Exception as e:
        logger.error(f"âŒ äººå£°åˆ†ç¦»å‡ºé”™: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return None

def run_speech_recognition(vocal_path):
    """è¿è¡Œè¯­éŸ³è¯†åˆ«"""
    logger.info("ğŸ¤ [æ­¥éª¤2] è¿è¡Œè¯­éŸ³è¯†åˆ« (WhisperX)...")
    
    try:
        from tools.step021_asr_whisperx import whisperx_transcribe_audio
        
        logger.info("ğŸ”„ æ‰§è¡Œè¯­éŸ³è¯†åˆ«...")
        start_time = time.time()
        transcript = whisperx_transcribe_audio(
            wav_path=vocal_path,
            model_name='large',
            device='mps',  # ä½¿ç”¨MPSåŠ é€Ÿ
            diarization=True
        )
        end_time = time.time()
        
        if transcript:
            logger.success(f"âœ… è¯­éŸ³è¯†åˆ«å®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’")
            logger.info(f"   è¯†åˆ«åˆ° {len(transcript)} æ¡å­—å¹•")
            
            # æ˜¾ç¤ºå‰å‡ æ¡å­—å¹•
            logger.info("ğŸ“ å­—å¹•é¢„è§ˆ:")
            for i, item in enumerate(transcript[:3]):
                text = item.get('text', '')[:50] + ('...' if len(item.get('text', '')) > 50 else '')
                logger.info(f"   [{i+1}] {item.get('start', 0):.2f}-{item.get('end', 0):.2f}s: {text}")
                if 'speaker' in item:
                    logger.info(f"       è¯´è¯äºº: {item['speaker']}")
            
            # ä¿å­˜å­—å¹•
            output_dir = Path(vocal_path).parent.parent
            subtitle_file = output_dir / "transcript.json"
            with open(subtitle_file, 'w', encoding='utf-8') as f:
                json.dump(transcript, f, ensure_ascii=False, indent=2)
            logger.info(f"ğŸ“„ å­—å¹•å·²ä¿å­˜åˆ°: {subtitle_file}")
            
            return transcript
        else:
            logger.error("âŒ è¯­éŸ³è¯†åˆ«è¿”å›ç©ºç»“æœ")
            return None
            
    except Exception as e:
        logger.error(f"âŒ è¯­éŸ³è¯†åˆ«å‡ºé”™: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return None

def run_translation(transcript):
    """è¿è¡Œç¿»è¯‘"""
    logger.info("ğŸ”¤ [æ­¥éª¤3] è¿è¡ŒAIç¿»è¯‘...")
    
    try:
        from tools.step035_translation_qwen import get_llm_api_config, llm_response
        
        # æ£€æŸ¥LLMé…ç½®
        api_key, base_url, model_name = get_llm_api_config()
        logger.info(f"âœ… å½“å‰LLMé…ç½®:")
        logger.info(f"   æ¨¡å‹: {model_name}")
        logger.info(f"   Base URL: {base_url}")
        
        if not transcript:
            logger.error("âŒ å­—å¹•æ•°æ®ä¸ºç©º")
            return None
        
        # å–å‰3å¥è¿›è¡Œç¿»è¯‘æ¼”ç¤º
        texts = [item['text'] for item in transcript[:3] if item.get('text', '').strip()]
        if not texts:
            logger.error("âŒ æ²¡æœ‰å¯ç¿»è¯‘çš„æ–‡æœ¬")
            return None
        
        text_batch = "\n".join(texts)
        logger.info(f"ğŸ”„ ç¿»è¯‘ {len(texts)} å¥æ–‡æœ¬...")
        
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„è§†é¢‘å­—å¹•ç¿»è¯‘å‘˜ã€‚è¯·å°†ä»¥ä¸‹è‹±æ–‡æ–‡æœ¬ç¿»è¯‘ä¸ºç®€ä½“ä¸­æ–‡ï¼Œä¿æŒè¯­å¢ƒè¿è´¯ï¼š

{text_batch}"""
        
        messages = [{"role": "user", "content": prompt}]
        
        start_time = time.time()
        translation = llm_response(messages)
        end_time = time.time()
        
        if translation:
            logger.success(f"âœ… ç¿»è¯‘å®Œæˆ! è€—æ—¶: {end_time - start_time:.2f}ç§’")
            logger.info(f"ğŸ“ ç¿»è¯‘ç»“æœé¢„è§ˆ: {translation[:100]}...")
            
            # ä¿å­˜ç¿»è¯‘ç»“æœ
            output_dir = Path(transcript[0]['text']).parent.parent if isinstance(transcript[0]['text'], str) else Path.cwd()
            translation_file = output_dir / "translation_result.txt"
            with open(translation_file, 'w', encoding='utf-8') as f:
                f.write(f"åŸæ–‡:\n{text_batch}\n\nè¯‘æ–‡:\n{translation}")
            logger.info(f"ğŸ“„ ç¿»è¯‘ç»“æœå·²ä¿å­˜åˆ°: {translation_file}")
            
            return translation
        else:
            logger.error("âŒ ç¿»è¯‘è¿”å›ç©ºç»“æœ")
            return None
            
    except Exception as e:
        logger.error(f"âŒ ç¿»è¯‘å‡ºé”™: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return None

def run_emotion_tts(transcript, vocal_path):
    """è¿è¡Œæƒ…ç»ªé…éŸ³"""
    logger.info("ğŸ™ï¸ [æ­¥éª¤4] è¿è¡Œæƒ…ç»ªé…éŸ³ (Cinecast)...")
    
    try:
        from tools.step045_tts_cinecast import generate_tts_with_emotion_clone
        
        if not transcript or not vocal_path:
            logger.error("âŒ ç¼ºå°‘å¿…è¦æ•°æ®")
            return False
        
        output_dir = Path(vocal_path).parent.parent / "dubbing_output"
        output_dir.mkdir(exist_ok=True)
        
        success_count = 0
        total_count = min(len(transcript), 2)  # æµ‹è¯•å‰2å¥
        
        logger.info(f"ğŸ”„ ç”Ÿæˆ {total_count} å¥æƒ…ç»ªé…éŸ³...")
        
        for i, item in enumerate(transcript[:total_count]):
            text = item.get('text', '').strip()
            start_time = item.get('start', 0)
            end_time = item.get('end', start_time + 3)
            
            if not text:
                continue
            
            output_file = output_dir / f"dub_{i:04d}.mp3"
            
            logger.info(f"ğŸ”Š ç”Ÿæˆç¬¬{i+1}å¥æƒ…ç»ªé…éŸ³: '{text[:30]}...'")
            
            success = generate_tts_with_emotion_clone(
                text=text,
                start_time=start_time,
                end_time=end_time,
                vocal_audio_path=vocal_path,
                output_audio_path=str(output_file),
                emotion_voice="aiden"
            )
            
            if success and output_file.exists():
                file_size = output_file.stat().st_size
                logger.success(f"âœ… ç¬¬{i+1}å¥é…éŸ³å®Œæˆ ({file_size} å­—èŠ‚)")
                success_count += 1
            else:
                logger.error(f"âŒ ç¬¬{i+1}å¥é…éŸ³å¤±è´¥")
        
        logger.info(f"ğŸ æƒ…ç»ªé…éŸ³å®Œæˆ: {success_count}/{total_count} å¥æˆåŠŸ")
        return success_count > 0
        
    except Exception as e:
        logger.error(f"âŒ æƒ…ç»ªé…éŸ³å‡ºé”™: {e}")
        logger.exception("è¯¦ç»†é”™è¯¯ä¿¡æ¯:")
        return False

def generate_final_report(results):
    """ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š"""
    logger.info("ğŸ“‹ ç”Ÿæˆé¦–ç§€æµ‹è¯•æŠ¥å‘Š...")
    
    report = {
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
        "test_video": "/Users/yuanliang/Downloads/p.mp4",
        "steps": {
            "demucs_separation": results[0],
            "speech_recognition": results[1],
            "translation": results[2],
            "emotion_tts": results[3]
        },
        "summary": {}
    }
    
    # è®¡ç®—æˆåŠŸç‡
    total_steps = len(results)
    passed_steps = sum(1 for result in results if result)
    success_rate = (passed_steps / total_steps) * 100 if total_steps > 0 else 0
    
    report["summary"] = {
        "total_steps": total_steps,
        "successful_steps": passed_steps,
        "failed_steps": total_steps - passed_steps,
        "success_rate": f"{success_rate:.1f}%"
    }
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = project_root / "first_run_test_report.json"
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)
    
    logger.success("ğŸ‰ é¦–ç§€æµ‹è¯•å®Œæˆ!")
    logger.info("ğŸ“Š æµ‹è¯•ç»“æœæ±‡æ€»:")
    steps_names = ["äººå£°åˆ†ç¦»", "è¯­éŸ³è¯†åˆ«", "AIç¿»è¯‘", "æƒ…ç»ªé…éŸ³"]
    for i, (step_result, step_name) in enumerate(zip(results, steps_names)):
        status = "âœ… æˆåŠŸ" if step_result else "âŒ å¤±è´¥"
        logger.info(f"   {step_name}: {status}")
    
    logger.info(f"ğŸ“ˆ æ€»ä½“æˆåŠŸç‡: {passed_steps}/{total_steps} ({success_rate:.1f}%)")
    logger.info(f"ğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    
    return report

def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    setup_logging()
    logger.success("ğŸš€ VideoLanguage é¦–ç§€æµ‹è¯•å¼€å§‹!")
    logger.info("æµ‹è¯•è§†é¢‘: /Users/yuanliang/Downloads/p.mp4")
    logger.info("ç¯å¢ƒ: videolang (conda-forge, Python 3.10.19, MPSæ”¯æŒ)")
    
    results = [False, False, False, False]  # [demucs, whisperx, translation, tts]
    
    # 1. å‡†å¤‡æµ‹è¯•è§†é¢‘
    video_path = prepare_test_video()
    if not video_path:
        logger.error("âŒ è§†é¢‘å‡†å¤‡å¤±è´¥")
        generate_final_report(results)
        return
    
    # 2. äººå£°åˆ†ç¦»
    vocal_path = run_demucs_separation(video_path)
    results[0] = bool(vocal_path)
    
    # 3. è¯­éŸ³è¯†åˆ«
    if vocal_path:
        transcript = run_speech_recognition(vocal_path)
        results[1] = bool(transcript)
    else:
        logger.warning("â­ï¸  è·³è¿‡è¯­éŸ³è¯†åˆ«ï¼ˆç¼ºå°‘äººå£°è½¨é“ï¼‰")
    
    # 4. ç¿»è¯‘
    if results[1]:  # å¦‚æœè¯­éŸ³è¯†åˆ«æˆåŠŸ
        translation = run_translation(transcript)
        results[2] = bool(translation)
    else:
        logger.warning("â­ï¸  è·³è¿‡ç¿»è¯‘ï¼ˆç¼ºå°‘å­—å¹•æ•°æ®ï¼‰")
    
    # 5. æƒ…ç»ªé…éŸ³
    if results[1]:  # å¦‚æœè¯­éŸ³è¯†åˆ«æˆåŠŸ
        tts_success = run_emotion_tts(transcript, vocal_path)
        results[3] = tts_success
    else:
        logger.warning("â­ï¸  è·³è¿‡æƒ…ç»ªé…éŸ³ï¼ˆç¼ºå°‘å¿…è¦æ•°æ®ï¼‰")
    
    # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
    generate_final_report(results)

if __name__ == "__main__":
    main()