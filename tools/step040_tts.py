import json
import os
import re
import librosa

from loguru import logger
import numpy as np

from .utils import save_wav, save_wav_norm
# --- é‡ç‚¹ä¿®æ”¹åŒºåŸŸå¼€å§‹ ---
# å°†ä¸‹é¢è¿™äº›åŸæœ‰çš„å†—ä½™ TTS å¼•æ“å…¨éƒ¨æ³¨é‡Šæ‰ï¼Œé˜²æ­¢å®ƒä»¬è§¦å‘åº•å±‚çš„ ImportError
# from .step041_tts_bytedance import tts as bytedance_tts  # éœ€è¦bytedanceä¾èµ–
# from .step042_tts_xtts import tts as xtts_tts  # éœ€è¦Coqui TTSåº“
# from .step043_tts_cosyvoice import tts as cosyvoice_tts  # éœ€è¦CosyVoiceä¾èµ–
from .step044_tts_edge_tts import tts as edge_tts  # Edge-TTSé€šå¸¸å¯ç”¨
from .step045_tts_cinecast import generate_tts_with_emotion_clone  # æˆ‘ä»¬çš„æ ¸å¿ƒCinecast TTSæ¨¡å—
# --- é‡ç‚¹ä¿®æ”¹åŒºåŸŸç»“æŸ ---
from .cn_tx import TextNorm
from audiostretchy.stretch import stretch_audio
normalizer = TextNorm()
def preprocess_text(text):
    text = text.replace('AI', 'äººå·¥æ™ºèƒ½')
    text = re.sub(r'(?<!^)([A-Z])', r' \1', text)
    text = normalizer(text)
    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åœ¨å­—æ¯å’Œæ•°å­—ä¹‹é—´æ’å…¥ç©ºæ ¼
    text = re.sub(r'(?<=[a-zA-Z])(?=\d)|(?<=\d)(?=[a-zA-Z])', ' ', text)
    return text
    
    
def adjust_audio_length(wav_path, desired_length, sample_rate = 24000, min_speed_factor = 0.6, max_speed_factor = 1.1):
    try:
        wav, sample_rate = librosa.load(wav_path, sr=sample_rate)
    except Exception as e:
        if wav_path.endswith('.wav'):
            wav_path = wav_path.replace('.wav', '.mp3')
        wav, sample_rate = librosa.load(wav_path, sr=sample_rate)
    current_length = len(wav)/sample_rate
    speed_factor = max(
        min(desired_length / current_length, max_speed_factor), min_speed_factor)
    logger.info(f"Speed Factor {speed_factor}")
    desired_length = current_length * speed_factor
    if wav_path.endswith('.wav'):
        target_path = wav_path.replace('.wav', f'_adjusted.wav')
    elif wav_path.endswith('.mp3'):
        target_path = wav_path.replace('.mp3', f'_adjusted.wav')
    stretch_audio(wav_path, target_path, ratio=speed_factor, sample_rate=sample_rate)
    wav, sample_rate = librosa.load(target_path, sr=sample_rate)
    return wav[:int(desired_length*sample_rate)], desired_length

tts_support_languages = {
    # XTTS-v2 supports 17 languages: English (en), Spanish (es), French (fr), German (de), Italian (it), Portuguese (pt), Polish (pl), Turkish (tr), Russian (ru), Dutch (nl), Czech (cs), Arabic (ar), Chinese (zh-cn), Japanese (ja), Hungarian (hu), Korean (ko) Hindi (hi).
    'xtts': ['ä¸­æ–‡', 'English', 'Japanese', 'Korean', 'French', 'Polish', 'Spanish'],
    'bytedance': [],
    'GPTSoVits': [],
    'EdgeTTS': ['ä¸­æ–‡', 'English', 'Japanese', 'Korean', 'French', 'Polish', 'Spanish'],
    # zero_shot usage, <|zh|><|en|><|jp|><|yue|><|ko|> for Chinese/English/Japanese/Cantonese/Korean
    'cosyvoice': ['ä¸­æ–‡', 'ç²¤è¯­', 'English', 'Japanese', 'Korean', 'French'], 
}

def generate_wavs(method, folder, target_language='ä¸­æ–‡', voice = 'zh-CN-XiaoxiaoNeural'):
    # å¼ºåˆ¶å°†æ— å…³çš„ TTS æ–¹æ³•åŠ«æŒæˆ–æŠ¥é”™
    if method == 'Cinecast':
        # è°ƒç”¨Cinecastæƒ…ç»ªé…éŸ³åŠŸèƒ½
        from .step045_tts_cinecast import generate_tts_with_emotion_clone
        # è¿™é‡Œéœ€è¦å®ç°ä¸åŸæœ‰é€»è¾‘å…¼å®¹çš„è°ƒç”¨
        logger.info("âœ… ä½¿ç”¨Cinecastè¿›è¡Œæƒ…ç»ªé…éŸ³")
        # TODO: å®ç°ä¸generate_wavsæ¥å£å…¼å®¹çš„Cinecastè°ƒç”¨
        pass
    elif method == 'EdgeTTS':
        # å¦‚æœæ‚¨è¿˜æƒ³ä¿ç•™å¾®è½¯å…è´¹TTSä½œä¸ºå¤‡ç”¨ï¼Œå¯ä»¥ç•™ç€å®ƒ
        logger.info("â„¹ï¸  ä½¿ç”¨EdgeTTSä½œä¸ºå¤‡ç”¨")
        pass
    else:
        # ç›´æ¥æŠ›å‡ºé”™è¯¯ï¼Œé˜²æ­¢å®ƒå»åŠ è½½å¸è½½äº†çš„ XTTS ç­‰æ¨¡å‹
        raise ValueError(f"âŒ ç³»ç»Ÿå·²å‡çº§çº¯å‡€ç‰ˆï¼Œä¸æ”¯æŒ {method}ï¼è¯·åœ¨ç•Œé¢é€‰æ‹© Cinecastã€‚")
    
    assert method in ['Cinecast', 'EdgeTTS']
    transcript_path = os.path.join(folder, 'translation.json')
    output_folder = os.path.join(folder, 'wavs')
    if not os.path.exists(output_folder):
        os.makedirs(output_folder)
    with open(transcript_path, 'r', encoding='utf-8') as f:
        transcript = json.load(f)
    speakers = set()
    
    for line in transcript:
        speakers.add(line['speaker'])
    num_speakers = len(speakers)
    logger.info(f'Found {num_speakers} speakers')

    # æ³¨é‡Šæ‰è¯­è¨€æ”¯æŒæ£€æŸ¥ï¼Œå…è®¸æ‰€æœ‰è¯­è¨€é€šè¿‡
    # if target_language not in tts_support_languages.get(method, []):
    #     logger.error(f'{method} does not support {target_language}')
    #     return f'{method} does not support {target_language}'
        
    full_wav = np.zeros((0, ))
    for i, line in enumerate(transcript):
        speaker = line['speaker']
        text = preprocess_text(line['translation'])
        output_path = os.path.join(output_folder, f'{str(i).zfill(4)}.wav')
        speaker_wav = os.path.join(folder, 'SPEAKER', f'{speaker}.wav')
        
        # åœ¨è°ƒç”¨ generate_tts_cinecast æˆ– edge_tts ä¹‹å‰åŠ ä¸Šï¼š
        if os.path.exists(output_path):  # output_path æ˜¯å½“å‰è¿™ä¸€å¥å°†è¦ä¿å­˜çš„ mp3 è·¯å¾„
            logger.info(f"â­ï¸ é…éŸ³å·²å­˜åœ¨ï¼Œè·³è¿‡: {output_path}")
            success = True  # æ ‡è®°ä¸ºæˆåŠŸï¼Œç»§ç»­å¤„ç†
        else:
            if method == 'Cinecast':
                # è°ƒç”¨Cinecast APIè¿›è¡Œæƒ…ç»ªé…éŸ³
                success = generate_tts_with_emotion_clone(
                    text=text,
                    start_time=line['start'],
                    end_time=line['end'],
                    vocal_audio_path=os.path.join(folder, 'audio_vocals.wav'),
                    output_audio_path=output_path,
                    emotion_voice="aiden"  # é»˜è®¤éŸ³è‰²
                )
                if not success:
                    logger.error(f"âŒ Cinecasté…éŸ³å¤±è´¥: {text}")
                    continue
            elif method == 'EdgeTTS':
                edge_tts(text, output_path, target_language = target_language, voice = voice)
                success = True
        
        start = line['start']
        end = line['end']
        length = end-start
        last_end = len(full_wav)/24000
        if start > last_end:
            full_wav = np.concatenate((full_wav, np.zeros((int((start - last_end) * 24000), ))))
        start = len(full_wav)/24000
        line['start'] = start
        if i < len(transcript) - 1:
            next_line = transcript[i+1]
            next_end = next_line['end']
            end = min(start + length, next_end)
        wav, length = adjust_audio_length(output_path, end-start)

        full_wav = np.concatenate((full_wav, wav))
        line['end'] = start + length
        
    vocal_wav, sr = librosa.load(os.path.join(folder, 'audio_vocals.wav'), sr=24000)
    
    # ã€æ·»åŠ è¿™é‡Œçš„ä¿æŠ¤ä»£ç ã€‘
    if len(full_wav) == 0 or np.max(np.abs(full_wav)) == 0:
        logger.error("âŒ æ‰€æœ‰TTSç”Ÿæˆå‡å¤±è´¥æˆ–ä¸ºç©ºï¼Œè·³è¿‡éŸ³é¢‘åˆå¹¶ï¼")
        return None, None
        
    # åŸæœ¬çš„ä»£ç ï¼š
    full_wav = full_wav / np.max(np.abs(full_wav)) * np.max(np.abs(vocal_wav))
    save_wav(full_wav, os.path.join(folder, 'audio_tts.wav'))
    with open(transcript_path, 'w', encoding='utf-8') as f:
        json.dump(transcript, f, indent=2, ensure_ascii=False)
    
    # --- æ™ºèƒ½å¯»æ‰¾ä¼´å¥æ–‡ä»¶ ---
    instruments_path = os.path.join(folder, 'audio_instruments.wav')
    # å¦‚æœæ‰¾ä¸åˆ°é»˜è®¤çš„ä¼´å¥æ–‡ä»¶ï¼Œå»å°è¯•æ‰¾ no_vocals.wav
    if not os.path.exists(instruments_path):
        # å…¼å®¹ Demucs çš„é»˜è®¤è¾“å‡ºè·¯å¾„
        demucs_bgm_path = os.path.join(folder, 'htdemucs_ft', 'download', 'no_vocals.wav')
        # å…¼å®¹å¯èƒ½å·²ç»è¢«å¤åˆ¶åˆ°æ ¹ç›®å½•çš„æƒ…å†µ
        root_bgm_path = os.path.join(folder, 'no_vocals.wav')
        
        if os.path.exists(root_bgm_path):
            instruments_path = root_bgm_path
        elif os.path.exists(demucs_bgm_path):
            instruments_path = demucs_bgm_path
        else:
            logger.error(f"âŒ æ‰¾ä¸åˆ°ä¼´å¥æ–‡ä»¶ï¼è¯·æ£€æŸ¥ {folder} ä¸‹æ˜¯å¦æœ‰ no_vocals.wav")
            return None, None
            
    logger.info(f"ğŸµ åŠ è½½èƒŒæ™¯ä¼´å¥: {instruments_path}")
    instruments_wav, sr = librosa.load(instruments_path, sr=24000)
    # --- æ™ºèƒ½å¯»æ‰¾ä¼´å¥æ–‡ä»¶ç»“æŸ ---
    len_full_wav = len(full_wav)
    len_instruments_wav = len(instruments_wav)
    
    if len_full_wav > len_instruments_wav:
        # å¦‚æœ full_wav æ›´é•¿ï¼Œå°† instruments_wav å»¶ä¼¸åˆ°ç›¸åŒé•¿åº¦
        instruments_wav = np.pad(
            instruments_wav, (0, len_full_wav - len_instruments_wav), mode='constant')
    elif len_instruments_wav > len_full_wav:
        # å¦‚æœ instruments_wav æ›´é•¿ï¼Œå°† full_wav å»¶ä¼¸åˆ°ç›¸åŒé•¿åº¦
        full_wav = np.pad(
            full_wav, (0, len_instruments_wav - len_full_wav), mode='constant')
    combined_wav = full_wav + instruments_wav
    # combined_wav /= np.max(np.abs(combined_wav))
    save_wav_norm(combined_wav, os.path.join(folder, 'audio_combined.wav'))
    logger.info(f'Generated {os.path.join(folder, "audio_combined.wav")}')
    return os.path.join(folder, 'audio_combined.wav'), os.path.join(folder, 'audio.wav')

def generate_all_wavs_under_folder(root_folder, method, target_language='ä¸­æ–‡', voice = 'zh-CN-XiaoxiaoNeural'):
    wav_combined, wav_ori = None, None
    for root, dirs, files in os.walk(root_folder):
        if 'translation.json' in files and 'audio_combined.wav' not in files:
            wav_combined, wav_ori = generate_wavs(method, root, target_language, voice)
        elif 'audio_combined.wav' in files:
            wav_combined, wav_ori = os.path.join(root, 'audio_combined.wav'), os.path.join(root, 'audio.wav')
            logger.info(f'Wavs already generated in {root}')
    return f'Generated all wavs under {root_folder}', wav_combined, wav_ori

if __name__ == '__main__':
    folder = r'videos/æ‘é•¿å°é’“åŠ æ‹¿å¤§/20240805 è‹±æ–‡æ— å­—å¹• é˜¿é‡Œè¿™å°å­åœ¨æ°´åŸå¨å°¼æ–¯å‘æ¥é—®å€™'
    generate_wavs('xtts', folder)
